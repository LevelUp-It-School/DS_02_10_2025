{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd0f38a",
   "metadata": {},
   "source": [
    "Задание: Кластеризация текстов и анализ представлений\n",
    "Цель работы\n",
    "\n",
    "Познакомиться с базовыми способами представления текстов в виде числовых векторов, кластеризацией и снижением размерности, а также увидеть влияние предобработки текста на результат.\n",
    "\n",
    "В работе:\n",
    "\n",
    "используются только тексты\n",
    "\n",
    "целевые метки не используются при обучении\n",
    "\n",
    "метки применяются только для визуального анализа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a60d1",
   "metadata": {},
   "source": [
    "Установка необходимых библиотек\n",
    "\n",
    "Установите необходимые пакеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da405f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.6)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.7.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: click in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\azubochenko\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\azubochenko\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\azubochenko\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.3 MB/s  0:00:00\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/28.0 MB 4.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.6/28.0 MB 3.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.4/28.0 MB 4.0 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.1/28.0 MB 4.0 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.9/28.0 MB 3.9 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.7/28.0 MB 4.0 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 5.5/28.0 MB 4.0 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 6.3/28.0 MB 4.0 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 7.1/28.0 MB 4.0 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.9/28.0 MB 4.0 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 8.9/28.0 MB 4.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 9.7/28.0 MB 4.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 10.5/28.0 MB 4.0 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.3/28.0 MB 4.0 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 12.1/28.0 MB 4.0 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 12.8/28.0 MB 4.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 13.6/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 14.2/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 14.9/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 15.7/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 16.5/28.0 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 17.3/28.0 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 18.1/28.0 MB 3.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 18.9/28.0 MB 3.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 19.7/28.0 MB 3.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 20.4/28.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 21.2/28.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 22.0/28.0 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 22.8/28.0 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 23.6/28.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 25.2/28.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.0/28.0 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 26.7/28.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.5/28.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 3.8 MB/s  0:00:07\n",
      "Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, pyarrow, dill, nltk, multiprocess, datasets\n",
      "\n",
      "   ------ --------------------------------- 1/6 [pyarrow]\n",
      "   ------ --------------------------------- 1/6 [pyarrow]\n",
      "   ------ --------------------------------- 1/6 [pyarrow]\n",
      "   ------ --------------------------------- 1/6 [pyarrow]\n",
      "   ------ --------------------------------- 1/6 [pyarrow]\n",
      "   ------ --------------------------------- 1/6 [pyarrow]\n",
      "   ------ --------------------------------- 1/6 [pyarrow]\n",
      "   ------------- -------------------------- 2/6 [dill]\n",
      "   -------------------- ------------------- 3/6 [nltk]\n",
      "   -------------------- ------------------- 3/6 [nltk]\n",
      "   -------------------- ------------------- 3/6 [nltk]\n",
      "   -------------------- ------------------- 3/6 [nltk]\n",
      "   -------------------- ------------------- 3/6 [nltk]\n",
      "   -------------------- ------------------- 3/6 [nltk]\n",
      "   -------------------- ------------------- 3/6 [nltk]\n",
      "   -------------------------- ------------- 4/6 [multiprocess]\n",
      "   --------------------------------- ------ 5/6 [datasets]\n",
      "   --------------------------------- ------ 5/6 [datasets]\n",
      "   --------------------------------- ------ 5/6 [datasets]\n",
      "   --------------------------------- ------ 5/6 [datasets]\n",
      "   ---------------------------------------- 6/6 [datasets]\n",
      "\n",
      "Successfully installed datasets-4.4.2 dill-0.4.0 multiprocess-0.70.18 nltk-3.9.2 pyarrow-22.0.0 xxhash-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af405f",
   "metadata": {},
   "source": [
    "2. Загрузка датасета с Hugging Face\n",
    "\n",
    "Используйте следующий датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4c4aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 925/925 [00:00<00:00, 86663.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('billingsmoore/text-clustering-example-data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8a7b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The pickup truck was loaded with heavy constru...</td>\n",
       "      <td>cars and trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sports car accelerated down the highway at...</td>\n",
       "      <td>cars and trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trucks are commonly used to transport goods ac...</td>\n",
       "      <td>cars and trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The electric car is becoming more popular due ...</td>\n",
       "      <td>cars and trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUVs are great for families who need extra spa...</td>\n",
       "      <td>cars and trucks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            topic\n",
       "0  The pickup truck was loaded with heavy constru...  cars and trucks\n",
       "1  The sports car accelerated down the highway at...  cars and trucks\n",
       "2  Trucks are commonly used to transport goods ac...  cars and trucks\n",
       "3  The electric car is becoming more popular due ...  cars and trucks\n",
       "4  SUVs are great for families who need extra spa...  cars and trucks"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ds — ваш DatasetDict\n",
    "train_ds = ds['train']\n",
    "\n",
    "# Преобразуем в DataFrame\n",
    "df = train_ds.to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddebdfb",
   "metadata": {},
   "source": [
    "Часть 2. Преобразование текста в векторы (без очистки)\n",
    "3. Bag of Words\n",
    "\n",
    "Реализуйте алгоритм преобразования текстов в числовые векторы.\n",
    "\n",
    "Алгоритм:\n",
    "\n",
    "Соберите словарь всех уникальных слов корпуса\n",
    "\n",
    "Каждому слову сопоставьте индекс\n",
    "\n",
    "Для каждого текста:\n",
    "\n",
    "создайте вектор длины |словарь|\n",
    "\n",
    "значение в каждой позиции — количество вхождений соответствующего слова в текст\n",
    "\n",
    "В результате каждый текст должен быть представлен числовым вектором фиксированной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb524e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77e98556",
   "metadata": {},
   "source": [
    "Часть 3. Кластеризация и визуализация\n",
    "4. Кластеризация\n",
    "\n",
    "Примените алгоритм кластеризации к векторам текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c29f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d60faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba2dc80",
   "metadata": {},
   "source": [
    "5. Снижение размерности\n",
    "\n",
    "Так как размерность векторов велика, примените метод снижения размерности до двух измерений (например, PCA или t-SNE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0c728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "007f299f",
   "metadata": {},
   "source": [
    "6. Визуализация\n",
    "\n",
    "Постройте две визуализации:\n",
    "\n",
    "Точки в 2D, раскрашенные по полученным кластерам\n",
    "\n",
    "Те же точки, раскрашенные по реальным меткам классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4b767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d982b6a",
   "metadata": {},
   "source": [
    "Часть 4. Предобработка текста\n",
    "\n",
    "Теперь повторите эксперимент после очистки текстов.\n",
    "\n",
    "7. Очистка текста\n",
    "\n",
    "Для каждого текста выполните:\n",
    "\n",
    "приведение к нижнему регистру\n",
    "\n",
    "удаление пунктуации и специальных символов\n",
    "\n",
    "удаление чисел (по желанию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bf980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "744c642c",
   "metadata": {},
   "source": [
    "8. Удаление стоп-слов\n",
    "\n",
    "Используйте список английских стоп-слов из nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2986fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32a638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fee6539",
   "metadata": {},
   "source": [
    "9. Нормализация слов\n",
    "\n",
    "После токенизации текста приведите слова к базовой форме, используя один из следующих подходов.\n",
    "\n",
    "Вариант 1. Стемминг\n",
    "\n",
    "Инициализируйте стеммер\n",
    "\n",
    "Примените его к каждому слову в тексте\n",
    "\n",
    "Используйте полученные формы при построении словаря\n",
    "\n",
    "Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = text.split()\n",
    "tokens = [stemmer.stem(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bf295",
   "metadata": {},
   "source": [
    "Вариант 2. Лемматизация\n",
    "\n",
    "Инициализируйте лемматизатор\n",
    "\n",
    "Примените его к каждому слову в тексте\n",
    "\n",
    "Используйте леммы при построении словаря\n",
    "\n",
    "Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = text.split()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9a406",
   "metadata": {},
   "source": [
    "Используйте только один из подходов и применяйте его ко всем текстам корпуса перед повторной векторизацией."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854358da",
   "metadata": {},
   "source": [
    "Часть 5. Повтор эксперимента\n",
    "10. Повторная векторизация\n",
    "\n",
    "Заново:\n",
    "\n",
    "постройте словарь\n",
    "\n",
    "создайте векторы текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed57a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e04472",
   "metadata": {},
   "source": [
    "11. Повторная кластеризация и визуализация\n",
    "\n",
    "Снова выполните:\n",
    "\n",
    "кластеризацию\n",
    "\n",
    "снижение размерности\n",
    "\n",
    "две визуализации:\n",
    "\n",
    "раскраска по кластерам\n",
    "\n",
    "раскраска по реальным меткам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83d074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f7ce84",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
